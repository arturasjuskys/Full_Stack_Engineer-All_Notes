# Database Maintenance
# PostgreSQL Database Maintenance

## Introduction
When you download a .csv file from the internet, you might designate a download location and then write a program to pull data from that location for analysis.

Although there are significant differences between accessing data from a .csv and a PostgreSQL database, the data you’re accessing is still stored on disk. Because PostgreSQL stores data on disk, you should take care to manage that storage like any other set of files. Just as you might only want to keep the most recent version of a large .csv file on your computer, you’ll want to keep your PostgreSQL database consuming only the disk space it needs.

The space PostgreSQL uses on disk can grow in several ways. Some ways are easier to predict, for example, the addition of new tables or the addition of more data to a table. However, there are some properties of the PostgreSQL data storage system that cause disk usage to increase in non-intuitive ways. For example, table size can increase after UPDATE statements, or a DELETE statement that removes millions of rows can result in no change in total table size.

In this lesson we’ll discuss some of the ways that you can manage the size of your database tables. Keeping the tables and indexes in your database small can ensure better query performance, more efficient disk utilization, and lower database costs.

## Understanding Object Size
In order to manage database disk utilization, you should first be able to measure disk utilization. As a database user, you can use the following functions to check the size of a relation in a database.
* pg_total_relation_size will return the size of the table and all its indexes in bytes. These values are often in the millions or billions and thus hard to read.
* pg_table_size and pg_indexes_size return the size of the table’s data and table’s indexes in bytes. The sum of these two functions is equal to pg_total_relation_size
* pg_size_pretty can be used with the functions above to format a number in bytes as KB, MB, or GB.

All of the logic to get the table or index size is in the function itself, although PostgreSQL does store table size in specific internal tables, you can simply write a statement like SELECT pg_table_size(table_name); to get a table’s size. Let’s look into an example using the table time_series.
```SQL
SELECT 
    pg_size_pretty(pg_table_size('time_series')) as tbl_size, 
    pg_size_pretty(pg_indexes_size('time_series')) as idx_size,
    pg_size_pretty(pg_total_relation_size('time_series')) as total_size;
```

From this query, we can see that the table has an index with a size of about 180KB, data size of about 350KB, and a total size equal to the sum.

| tbl_size | idx_size | total_size |
| -------- | -------- | ---------- |
| 352KB | 184KB | 536KB |

Because indexes are relations in their own right, you can also call pg_total_relation_size on a single index to get the size of the index. For example, if we know that time_series has an index named pk_mocked_data, we can check the total relation size of that index with the following.
```SQL
SELECT pg_size_pretty(
     pg_total_relation_size('pk_mocked_data')
) as idx_size;
```

| idx_size |
| -------- |
| 184 kB |

One other thing to consider is that in PostgreSQL, when you SELECT a function value, the column in the response will have the same name as the function. For clarity, it can help to include ... as \<desired name> in your queries, otherwise columns in the response would be named pg_size_pretty.

## Updates and Table Size
Each row in a PostgreSQL table is stored in a file on the disk of the host machine. When an UPDATE or DELETE is called, PostgreSQL doesn’t physically delete the content from the disk. Instead, the database engine marks those rows so that they aren’t returned in user queries. These rows are called dead tuples, and although they aren’t referenced in the current version of databases’ tables, they still occupy space on disk and can affect performance.

Imagine a table, sensordata, that receives values from dozens of temperature sensors every second. Occasionally, the sensors will write multiple entries per second, adding an extra row. When this happens, the old entry for that second will be marked invalid.

| id | datetime | sensor_id | temp_c | invalid |
| -- | -------- | --------- | ------ | ------- |
| 1 | 01-01-2021 | 00:03:01 | 1 | 14.56	 | |
| 2 | 01-01-2021 | 00:03:01 | 2 | 15.92 | True |
| 3 | 01-01-2021 | 00:03:01 | 3 | 12.72	 | |
| 4 | 01-01-2021 | 00:03:01 | 2 | 15.88	 | |

Over time, querying this table for valid rows will become more expensive because PostgreSQL will spend more time scanning over potentially millions of irrelevant rows. Keeping millions of unneeded rows could have a significant impact on query performance!

This example is a simplified version of what PostgreSQL does behind-the-scenes when you UPDATE a table. PostgreSQL creates a new row and marks an “old” row invalid, causing the size of the table to increase. Let’s turn to an example. Consider the table mock.time_series from earlier.
```SQL
SELECT pg_size_pretty(
    pg_total_relation_size('mock.time_series')
) as total_size;
```

| total_size |
| ---------- |
| 4328 kB |

Let’s perform an update to test how table size fluctuates. Because the column score was randomly generated between 0 and 1, the following should affect about 10% of the rows.
```SQL
UPDATE rand 
SET score = 1 
WHERE score > .9;
```

```SQL
> UPDATE 9965
```

Re-running pg_total_relation_size() on this table returns the following:

| total_size |
| ---------- |
| 4784 kB |

This may be somewhat unexpected. This query updated 9,965/100,000 (9.97%) of values, but didn’t add any new data to the table. So why is the table larger? The table grew about 456KB/4328KB (10.53%) because of the way that PostgreSQL handles updates internally; the additional 400KB or so are the dead tuples created from this update. In the following parts of this lesson we’ll discuss strategies to handle this increased disk usage.

## Introduction to VACUUM
In the last exercise we discussed ways to monitor disk usage. There are also statements you can use that allow you to actively manage disk usage. In PostgreSQL there is an operation called VACUUM that can be used to manage storage space. Running VACUUM \<table name>; will vacuum a specific table, while a VACUUM statement without a table name will run on the entire database.

If you only check pg_total_relation_size, you may not see much of a decrease in space before and after a vacuum. This is because a plain VACUUM will only clear tables’ dead tuples where possible. Depending on which rows in your table are updated, this can clear anywhere between 0 and 100% of dead tuples. If VACUUM cannot clear the dead tuples, PostgreSQL will mark the space occupied by dead tuples for reuse when new data is inserted into the table. Later in this lesson, we’ll discuss parameters that can be passed along with VACUUM that promise to return more space to disk.

Recall from our previous example using time_series that when we updated our table with about 10,000 new values we also created about 400kB of dead tuples. Starting with the same table, let’s see how using VACUUM could help manage table size.
```SQL
SELECT pg_size_pretty(
    pg_total_relation_size('mock.time_series')
) as total_size;
```

| total_size |
| - |
| 4328 kB |

```SQL
UPDATE mock.time_series 
SET score = 0 
WHERE score < .1;
```
```SQL
>> UPDATE 10019
```
```SQL
SELECT pg_size_pretty(
    pg_total_relation_size('mock.time_series')
);
```

| total_size |
| - |
| 4784 kB |

After updating the table we see an increase in disk space used by this table. Now let’s consider how VACUUM can help us minimize the effect of new updates.
```SQL
VACUUM mock.time_series;
 
SELECT pg_size_pretty(
    pg_total_relation_size('mock.time_series')
);
```

| total_size |
| - |
| 4784 kB |

Unfortunately, we see that no space has been returned to disk. This is a bit disappointing, but what the VACUUM has done is allow this space to be overwritten by subsequent operations on that table. Let’s see what happens when we perform another update similar to the previous one.
```SQL
UPDATE mock.time_series 
SET score = 0 
WHERE score < .2 
AND score > .1;
```
```SQL
>> UPDATE 9927
```
```SQL
SELECT pg_size_pretty(
    pg_total_relation_size('mock.time_series')
);
```

| total_size |
| - |
| 4792 kB |

We see that the table has grown by only an additional 8kB instead of the 400kB that the first update produced. In this case, the space that we marked safe for overwriting with VACUUM was used so the table didn’t have to use much more space on disk.

## Analyze and Autovacuum
In previous lessons we’ve discussed the VACUUM command and it’s limitations. One nice property about VACUUM is that it allows space to be reused. If tables are vacuumed frequently enough, the disk usage of a table will stay relatively steady because updates will never get “too far ahead” of the required space on disk. To ensure that vacuuming isn’t left completely to the database users, PostgreSQL has a feature called autovacuum enabled on most databases by default. When using autovacuum, PostgreSQL periodically checks for tables that have had a large number of inserted, updated or deleted tuples that could be vacuumed to improve performance. When autovacuum is enabled and finds such a table, a VACUUM ANALYZE command is run. This statement is a combination of two separate operations.
* VACUUM, which manages the dead tuples in a database table
* ANALYZE, which is a statement that allows PostgreSQL look at a table and gather information about contents. PostgreSQL then stores this data internally and uses it to ensure that queries are planned in the most efficient way given the structure of the table.

You can leave running VACUUM and ANALYZE statements to the autovacuum process if you’ like. However, you can also run it yourself with VACUUM ANALYZE \<table name>; or just ANALYZE \<table name> if you haven’t made large inserts or updates and would like to update pg_stat_all_tables.

You can monitor the last vacuum or autovacuum by querying the table pg_stat_all_tables for vacuum and analyze statistics. pg_stat_all_tables is table that contains internal PostgreSQL statistics; you can query for a specific table’s statistics by filtering on the column relname (i.e. relation name).

For example, the following query could be use to check statistics for a table named books
```SQL
SELECT relname, 
    last_vacuum,
    last_autovacuum, 
    last_analyze
FROM pg_stat_all_tables 
WHERE relname = 'books';
```

| relname | last_vacuum | last_autovacuum | last_analyze |
| - | - | - | - |
| books | 2021-02-01 00:13:14 | 2021-02-01 00:00:00 | 2021-02-01 00:13:14 |

In the example above, from the last_autovacuum column we can tell that a autovaccum ran at midnight on 2021-02-01. From the last_vacuum and last_analyze columns we can see that several minutes later a user manually ran a VACUUM ANALYZE

pg_stat_all_tables contains quite a few useful fields; you can see a full list of the fields this table contains [here](https://www.postgresql.org/docs/12/monitoring-stats.html#PG-STAT-ALL-TABLES-VIEW).

## Deletes and Table Size
As you saw in a previous exercises with UPDATE, even when the total number of rows stored in the table is unchanged, disk utilization can increase. The behavior for DELETE operations is slightly different. Unlike updates, deletes don’t add space to a table. However, a DELETE statement will create dead tuples and leave the size of the table unchanged.

Imagine we have a table,promotions.contest_entries that contains 100,000 rows of data used for a virtual lottery drawing, and we’d like to run a query nightly to remove entrants that have not logged in since the contest started.

| user_id | contest_id | num_logins |
| - | - | - |
| 1 | 1 | 8 |
| 2 | 1 | 12 |
| 3 | 1 | 0 |

```SQL
SELECT pg_size_pretty(
    pg_total_relation_size('contest_entries')
) as total_size;
```

| total_size |
| - |
| 784 kB |

```SQL
DELETE FROM contest_entries WHERE num_logins = 0
```
```SQL
>> Delete 1000
```

Re-running pg_total_relation_size() on this table returns the following.

| total_size |
| - |
| 784 kB |

The size of a table alone doesn’t give us all the information we need about maintenance status. We can query pg_stat_all_tables to help us understand the status of tuples in a table. Specifically, we can use the columns n_dead_tup, and n_live_tup from this table to asses the status of the table.
```SQL
SELECT schemaname, relname, n_dead_tup, n_live_tup
FROM pg_stat_all_tables LIMIT 3
```

| schemaname | relname | n_dead_tup | n_live_tup |
| - | - | - | - |
| promotions | contest_entries | 1000 | 99000 |
| promotions | contests | 0 | 25000 |
| promotions | prizes | 0 | 100 |

We can see that the update created 1000 dead tuples, the same as the number of rows we deleted. We can also use the column n_live_tup to determine that only 99,000 rows are being displayed (meaning that the 1000 deleted rows have been marked dead)

Depending on the settings on your database, you may need to ANALYZE \<table name> a table to see accurate statistics in pg_stat_all_tables.

In other cases, your database’s autovacuum might be triggered by an update, in which case you could expect a result like the below, where the dead tuples have already been cleared.

| schemaname | relname | n_dead_tup | n_live_tup |
| - | - | - | - |
| promotions | contest_entries | 0 | 99000 |

## Vacuum Full
There is an alternative VACUUM method, VACUUM FULL which rewrites all the data from a table into a “new” location on disk and only copies the required data (excluding dead tuples). This allows PostgreSQL to fully clear the space the table occupied. One of the significant drawbacks from VACUUM FULL is that it’s a slow operation that blocks other operations on the table while it’s working. If you’ve got a large table, this could mean a VACUUM FULL operation might block other user’s or application’s queries. In a local setting, this may seem trivial, but for production databases, preventing reads and writes on a table for even a few seconds can have lasting effects.

Although a plain VACUUM won’t reduce table size, plain VACUUM is designed to be able to run in parallel with normal reading and writing of the table. Let’s work through an example to demonstrate how VACUUM FULL can aggressively reduce table size.

We have a table (mock.rand) with 12 million rows of randomly generated values that takes up about 500MB on disk.
```SQL
SELECT pg_size_pretty(
    pg_total_relation_size('mock.rand')
) as total_size;
```

| total_size |
| - |
| 507 MB |

After running a large UPDATE on the table, re-running the query from above now gives a new total relation size of 559MB. Additionally, a query to pg_catalog.pg_stat_all_tables shows 1.2M dead tuples.
```SQL
select relname, n_dead_tup , last_vacuum
from pg_catalog.pg_stat_all_tables
where relname  = 'rand';
```

| relname | n_dead_tup | last_vacuum |
| rand | 1.2M |	NULL |

Running VACUUM on this table takes just about 1.5 seconds. Recall from the previous exercises that not much (if any) space will be returned to disk (i.e. pg_total_relation_size() will still give 559MB), but the table’s dead tuples will be marked for reuse.

Instead, if we use VACUUM FULL, PostgreSQL takes about 15s to scan the table and fully clear unneeded space. Running pg_total_relation_size() on the table now shows that the table size has been reduced back to its original size, 507MB.

While this was effective, VACUUM FULL is quite a heavy operation that should be used sparingly. The best strategy when designing a database maintenance plan is to make sure that VACUUM runs frequently and autovacuum is enabled. These measures will ensure that table sizes are relatively stable over time.

## Truncate
In an exercise earlier in this lesson, you learned that DELETE statements will not reduce the space a table occupies on disk. Because of the way that VACUUM works, even when using DELETE with VACUUM, there’s no guarantee that all the deleted tuples will be returned to disk. So far, the only way that we’ve discussed to completely reduce the disk usage of a table to the minimum required space is by calling VACUUM FULL. However, in some special cases, a VACUUM FULL can be avoided.

Occasionally, you may need to remove all the rows, but retain the structure of a table. In this situation, it wouldn’t be ideal to drop the table and re-populate it with data, so you might opt for what’s called an unqualified delete — a DELETE that affects all rows (e.g. DELETE * FROM table WHERE true;). In large tables, an unqualified delete can be quite expensive, as it still requires scanning all rows.

In these situations, one common solution is to use TRUNCATE. TRUNCATE quickly removes all rows from a table. It has the same effect as an unqualified delete, but since PostgreSQL doesn’t scan through the table first, TRUNCATE runs much faster on large tables. Finally, TRUNCATE simultaneously reclaims disk space immediately, rather than requiring a subsequent VACUUM or VACCUM FULL operation.

In the event that you need to clear a table, the following two sets of statements are equivalent, however the TRUNCATE uses fewer system resources and packs the logic of multiple statements into one. Consider the following example from a table with a 500MB table containing ~12 million randomly generated values.
```SQL
SELECT pg_size_pretty(
    pg_total_relation_size('rand')
) as total_size;
```

The following two queries take about 20 seconds total, ~17.5s to scan the table and perform the DELETE and another 2.5s to VACUUM FULL the dead tuples the delete left behind.
```SQL
DELETE * FROM rand WHERE true;
VACUUM FULL rand;
```

On the other hand, when we use TRUNCATE rand instead, the query runs for about 150 milliseconds.
```SQL
SELECT
    pg_size_pretty(pg_table_size('rand')) as table_size,
    pg_size_pretty(pg_indexes_size('rand')) as index_size,
    pg_size_pretty(pg_total_relation_size('rand')) as total_size;
```

| table_size | index_size | total_size |
| - | - | - |
| 0 bytes | 0 bytes | 0 bytes |

We’re left with the same result using both TRUNCATE and DELETE, however the TRUNCATE statement used far fewer system resources and completed far faster than the DELETE.

## Wrap-Up
This concludes your lesson on database maintenance; while it may not be the most glamorous subject, a well maintained database is the core of most robust applications. Some of the tools you can take with you include:
* How an UPDATE or DELETE creates dead tuples on disk. You also now have the ability to write queries to track disk usage of a table or index and assess the effect of dead tuples on a table.
* An understanding of when to VACUUM, VACUUM ANALYZE, or VACUUM FULL. Each can be used to use database space more efficiently when appropriate.
  * VACUUM simply marks dead tuples and allows that space to be re-used by future updates.
  * VACUUM ANALYZE performs a VACUUM, but also updates PostgreSQL’s internal statistics and can help further improve query performance after a large UPDATE.
  * VACUUM FULL is the most aggressive VACUUM strategy, it can completely clear dead tuples from a table and return the space to disk. However, it should be used sparingly as it can prevent other users from querying the table.
* Awareness of the autovacuum process, which automatically runs periodic VACCUUMs to keep database tables disk usage efficient.
* The ability to interpret data from pg_stat_all_tables. This is a very handy table. Being able to reference it to understand disk usage, vacuum frequency, or dead tuple count will allow you to diagnose a wide variety of database performance problems.
* An understanding of when TRUNCATE can be used in place of more expensive alternatives.
